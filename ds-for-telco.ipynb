{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data from: https://www.sgi.com/tech/mlc/db/churn.all\n",
    "\n",
    "Fields:\n",
    "\n",
    "state: discrete.\n",
    "account length: continuous.\n",
    "area code: continuous.\n",
    "phone number: discrete.\n",
    "international plan: discrete.\n",
    "voice mail plan: discrete.\n",
    "number vmail messages: continuous.\n",
    "total day minutes: continuous.\n",
    "total day calls: continuous.\n",
    "total day charge: continuous.\n",
    "total eve minutes: continuous.\n",
    "total eve calls: continuous.\n",
    "total eve charge: continuous.\n",
    "total night minutes: continuous.\n",
    "total night calls: continuous.\n",
    "total night charge: continuous.\n",
    "total intl minutes: continuous.\n",
    "total intl calls: continuous.\n",
    "total intl charge: continuous.\n",
    "number customer service calls: continuous.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "schema = StructType([ \\\n",
    "    StructField(\"state\", StringType(), True), \\\n",
    "    StructField(\"account_length\", DoubleType(), True), \\\n",
    "    StructField(\"area_code\", StringType(), True), \\\n",
    "    StructField(\"phone_number\", StringType(), True), \\\n",
    "    StructField(\"international_plan\", StringType(), True), \\\n",
    "    StructField(\"voice_mail_plan\", StringType(), True), \\\n",
    "    StructField(\"number_vmail_messages\", DoubleType(), True), \\\n",
    "    StructField(\"total_day_minutes\", DoubleType(), True), \\\n",
    "    StructField(\"total_day_calls\", DoubleType(), True), \\\n",
    "    StructField(\"total_day_charge\", DoubleType(), True), \\\n",
    "    StructField(\"total_eve_minutes\", DoubleType(), True), \\\n",
    "    StructField(\"total_eve_calls\", DoubleType(), True), \\\n",
    "    StructField(\"total_eve_charge\", DoubleType(), True), \\\n",
    "    StructField(\"total_night_minutes\", DoubleType(), True), \\\n",
    "    StructField(\"total_night_calls\", DoubleType(), True), \\\n",
    "    StructField(\"total_night_charge\", DoubleType(), True), \\\n",
    "    StructField(\"total_intl_minutes\", DoubleType(), True), \\\n",
    "    StructField(\"total_intl_calls\", DoubleType(), True), \\\n",
    "    StructField(\"total_intl_charge\", DoubleType(), True), \\\n",
    "    StructField(\"number_customer_service_calls\", DoubleType(), True), \\\n",
    "    StructField(\"churned\", StringType(), True)])\n",
    "\n",
    "df = sqlContext.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .load('churn.all', schema = schema)\n",
    "    \n",
    "# TODO: remove extraneous spaces from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = [\n",
    "        'number_customer_service_calls', \\\n",
    "        'total_night_minutes', \\\n",
    "        'total_day_minutes', \\\n",
    "        'total_eve_minutes', \\\n",
    "        'account_length'],\n",
    "    outputCol = 'features')\n",
    "\n",
    "feature_df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "label_func = udf(lambda c: 1.0 if c == \" True.\" else 0.0, DoubleType())\n",
    "train_set = feature_df.select('features', label_func(df.churned).alias('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "train_set.cache()\n",
    "model = lr.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.weights)\n",
    "print(model.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(train_set)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
